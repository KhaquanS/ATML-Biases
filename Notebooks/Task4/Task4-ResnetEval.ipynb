{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9494532,"sourceType":"datasetVersion","datasetId":5777290},{"sourceId":9494870,"sourceType":"datasetVersion","datasetId":5777556},{"sourceId":9499703,"sourceType":"datasetVersion","datasetId":5781146},{"sourceId":9499821,"sourceType":"datasetVersion","datasetId":5781243}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pulling in Imagenette","metadata":{}},{"cell_type":"code","source":"import os\nimport torchvision\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.RandAugment(),\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\nimagenette_train = torchvision.datasets.Imagenette(root='./', split='train', size='full', download=True, transform=transform)\nDATA_PATH = '../input/edges-data/content_imagenette/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T08:07:46.553371Z","iopub.execute_input":"2024-09-28T08:07:46.553662Z","iopub.status.idle":"2024-09-28T08:09:31.437060Z","shell.execute_reply.started":"2024-09-28T08:07:46.553630Z","shell.execute_reply":"2024-09-28T08:09:31.436261Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz to ./imagenette2.tgz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1557161267/1557161267 [01:26<00:00, 18004626.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./imagenette2.tgz to ./\n","output_type":"stream"}]},{"cell_type":"code","source":"class_map = {\n    'n03028079': 'church',\n    'n01440764': 'tench',\n    'n03000684': 'chain saw',\n    'n03425413': 'gas pump',\n    'n02979186': 'casette player',\n    'n02102040': 'English springer',\n    'n03417042': 'garbage truck',\n    'n03394916': 'French horn',\n    'n03888257': 'parachute',\n    'n03445777': 'golf ball'\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:31.438907Z","iopub.execute_input":"2024-09-28T08:09:31.439472Z","iopub.status.idle":"2024-09-28T08:09:31.444706Z","shell.execute_reply.started":"2024-09-28T08:09:31.439429Z","shell.execute_reply":"2024-09-28T08:09:31.443746Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nstyles = ['content_imagenette/content_imagenette', 'color', 'edges', 'stylized_imagenette/stylized_imagenette']\n\nfor style in styles:\n    folder_path = f'../input/stylized-data/{style}'\n    for folder in os.listdir(folder_path):\n        print(f'{style} {folder} has {len(os.listdir(os.path.join(folder_path, folder)))} images ....' )","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:31.445770Z","iopub.execute_input":"2024-09-28T08:09:31.446090Z","iopub.status.idle":"2024-09-28T08:09:31.889980Z","shell.execute_reply.started":"2024-09-28T08:09:31.446059Z","shell.execute_reply":"2024-09-28T08:09:31.889074Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"content_imagenette/content_imagenette chain saw has 50 images ....\ncontent_imagenette/content_imagenette English springer has 50 images ....\ncontent_imagenette/content_imagenette church has 50 images ....\ncontent_imagenette/content_imagenette French horn has 50 images ....\ncontent_imagenette/content_imagenette gas pump has 50 images ....\ncontent_imagenette/content_imagenette golf ball has 50 images ....\ncontent_imagenette/content_imagenette garbage truck has 50 images ....\ncontent_imagenette/content_imagenette cassette player has 50 images ....\ncontent_imagenette/content_imagenette parachute has 50 images ....\ncontent_imagenette/content_imagenette tench has 50 images ....\ncolor chain saw has 50 images ....\ncolor English springer has 50 images ....\ncolor church has 50 images ....\ncolor French horn has 50 images ....\ncolor gas pump has 50 images ....\ncolor golf ball has 50 images ....\ncolor garbage truck has 50 images ....\ncolor cassette player has 50 images ....\ncolor parachute has 50 images ....\ncolor tench has 50 images ....\nedges chain saw has 50 images ....\nedges English springer has 50 images ....\nedges church has 50 images ....\nedges French horn has 50 images ....\nedges gas pump has 50 images ....\nedges golf ball has 50 images ....\nedges garbage truck has 50 images ....\nedges cassette player has 50 images ....\nedges parachute has 50 images ....\nedges tench has 50 images ....\nstylized_imagenette/stylized_imagenette chain saw has 50 images ....\nstylized_imagenette/stylized_imagenette English springer has 50 images ....\nstylized_imagenette/stylized_imagenette church has 50 images ....\nstylized_imagenette/stylized_imagenette French horn has 50 images ....\nstylized_imagenette/stylized_imagenette gas pump has 50 images ....\nstylized_imagenette/stylized_imagenette golf ball has 50 images ....\nstylized_imagenette/stylized_imagenette garbage truck has 50 images ....\nstylized_imagenette/stylized_imagenette cassette player has 50 images ....\nstylized_imagenette/stylized_imagenette parachute has 50 images ....\nstylized_imagenette/stylized_imagenette tench has 50 images ....\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Applying Canny Filter","metadata":{}},{"cell_type":"code","source":"# import os\n# import cv2\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# def apply_canny_filter(image_path):\n#     image = cv2.imread(image_path)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n\n#     gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n#     edges = cv2.Canny(gray_image, threshold1=100, threshold2=200)\n#     edges_display = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)  # Convert to 3-channel for display\n    \n#     return edges_display\n\n# def process_images(input_folder, output_folder):\n#     os.makedirs(output_folder, exist_ok=True)\n\n#     for filename in os.listdir(input_folder):\n#         image_path = os.path.join(input_folder, filename)\n#         edges_display = apply_canny_filter(image_path)\n#         output_path = os.path.join(output_folder, filename)\n#         cv2.imwrite(output_path, cv2.cvtColor(edges_display, cv2.COLOR_RGB2BGR))  # Save as BGR\n    \n#     print(f\"Processed and saved: {class_map[input_folder.split('/')[-1]]}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:31.892678Z","iopub.execute_input":"2024-09-28T08:09:31.893305Z","iopub.status.idle":"2024-09-28T08:09:31.897827Z","shell.execute_reply.started":"2024-09-28T08:09:31.893269Z","shell.execute_reply":"2024-09-28T08:09:31.896887Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# import os\n# import cv2\n\n# output_root = os.path.join(os.getcwd(), 'edges')\n# input_root = DATA_PATH\n\n# # Create output root if it doesn't exist\n# if not os.path.exists(output_root):\n#     os.mkdir(output_root)\n\n# # Get input folders and corresponding output folders\n# input_folders = [os.path.join(input_root, folder) for folder in os.listdir(input_root)]\n# output_folders = [os.path.join(output_root, os.path.basename(folder)) for folder in input_folders]\n\n# for in_folder, out_folder in zip(input_folders, output_folders):\n#     # Create output subdirectory if it doesn't exist\n#     if not os.path.exists(out_folder):\n#         os.mkdir(out_folder)\n\n#     for filename in os.listdir(in_folder):\n#         image_path = os.path.join(in_folder, filename)\n#         edges_display = apply_canny_filter(image_path)\n\n#         # Ensure the output path is valid before saving\n#         output_path = os.path.join(out_folder, filename)\n#         cv2.imwrite(output_path, cv2.cvtColor(edges_display, cv2.COLOR_RGB2BGR))\n\n#     print(f\"Processed and saved: {os.path.basename(in_folder)} at {out_folder}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:31.898869Z","iopub.execute_input":"2024-09-28T08:09:31.899158Z","iopub.status.idle":"2024-09-28T08:09:31.912688Z","shell.execute_reply.started":"2024-09-28T08:09:31.899126Z","shell.execute_reply":"2024-09-28T08:09:31.911770Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning and Inference","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ntorch.manual_seed(42)\n\nmodel = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\nmodel.fc = nn.Linear(in_features=model.fc.in_features, out_features=10)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.fc.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:31.913806Z","iopub.execute_input":"2024-09-28T08:09:31.914909Z","iopub.status.idle":"2024-09-28T08:09:33.051998Z","shell.execute_reply.started":"2024-09-28T08:09:31.914862Z","shell.execute_reply":"2024-09-28T08:09:33.051040Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 172MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"val_dir = 'imagenette2/val'\ntrain_dir = 'imagenette2/train'\nfolders = os.listdir(train_dir)\n\nfor folder in folders:\n    if folder in class_map:\n        old_folder = os.path.join(train_dir, folder)\n        new_folder = os.path.join(train_dir, class_map[folder])\n        os.rename(old_folder, new_folder)\n        print(f'Renamed {folder} to {class_map[folder]}')\n\nfolders = os.listdir(val_dir)\n\nfor folder in folders:\n    if folder in class_map:\n        old_folder = os.path.join(val_dir, folder)\n        new_folder = os.path.join(val_dir, class_map[folder])\n        os.rename(old_folder, new_folder)\n        print(f'Renamed {folder} to {class_map[folder]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:33.053140Z","iopub.execute_input":"2024-09-28T08:09:33.053458Z","iopub.status.idle":"2024-09-28T08:09:33.062565Z","shell.execute_reply.started":"2024-09-28T08:09:33.053423Z","shell.execute_reply":"2024-09-28T08:09:33.061768Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Renamed n03394916 to French horn\nRenamed n02102040 to English springer\nRenamed n03000684 to chain saw\nRenamed n03425413 to gas pump\nRenamed n03445777 to golf ball\nRenamed n01440764 to tench\nRenamed n03028079 to church\nRenamed n02979186 to casette player\nRenamed n03888257 to parachute\nRenamed n03417042 to garbage truck\nRenamed n03394916 to French horn\nRenamed n02102040 to English springer\nRenamed n03000684 to chain saw\nRenamed n03425413 to gas pump\nRenamed n03445777 to golf ball\nRenamed n01440764 to tench\nRenamed n03028079 to church\nRenamed n02979186 to casette player\nRenamed n03888257 to parachute\nRenamed n03417042 to garbage truck\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 32\n\ntrain_data_path = 'imagenette2/train'\nnormal_path = '../input/stylized-data/content_imagenette/content_imagenette'\nedge_path = '../input/stylized-data/edges'\ncolor_path = '../input/stylized-data/color'\nshape_path = '../input/stylized-data/stylized_imagenette/stylized_imagenette'\n\n\n# Define the data transformations (you can adjust these as needed)\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandAugment(),\n        transforms.Resize((256, 256)),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ]),\n}\n\ntrain_dataset = datasets.ImageFolder(train_data_path, transform=data_transforms['train'])\ntest_dataset = datasets.ImageFolder('imagenette2/val', transform=data_transforms['val'])\n\nval_dataset = [datasets.ImageFolder(normal_path, transform=data_transforms['val']),\n               datasets.ImageFolder(edge_path, transform=data_transforms['val']),\n               datasets.ImageFolder(color_path, transform=data_transforms['val']),\n               datasets.ImageFolder(shape_path, transform=data_transforms['val']),\n              ]\n\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nnormal_loader = DataLoader(val_dataset[0] , batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\nedge_loader = DataLoader(val_dataset[1] , batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\ncolor_loader = DataLoader(val_dataset[2] , batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\nshape_loader = DataLoader(val_dataset[3] , batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f'Training samples: {len(train_dataset)}')\nprint(f'Validation samples: {len(val_dataset[0])}')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:33.063638Z","iopub.execute_input":"2024-09-28T08:09:33.063963Z","iopub.status.idle":"2024-09-28T08:09:33.308592Z","shell.execute_reply.started":"2024-09-28T08:09:33.063928Z","shell.execute_reply":"2024-09-28T08:09:33.307613Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training samples: 9469\nValidation samples: 500\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_step(model, dataloader, criterion, optimizer, device, name):\n    '''Train for one epoch'''\n    \n    model.train()\n\n    train_loss = 0.0\n    train_acc = 0.0\n\n    for i, data in enumerate(dataloader):\n        if name == 'PACS':\n            X = data['images']\n            y = torch.squeeze(data['labels'])\n\n            X = X.to(device)\n            y = y.to(device)\n\n        else:\n            X, y = data[0].to(device), data[1].to(device)\n        \n        logits = model(X)\n        loss = criterion(logits, y)\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += (y_pred == y).sum().item() / len(y)\n\n        # Print dynamic progress on the same line using \\r\n        print(f'\\rTraining: [{i+1}/{len(dataloader)}] '\n              f'Loss: {train_loss / (i + 1):.4f} '\n              f'Acc: {train_acc / (i + 1):.4f}', end='')\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    \n    # Move to the next line after the loop is done\n    print()  \n    \n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device, name):\n    '''Evaluate the model'''\n    \n    model.eval()\n\n    eval_loss = 0.0\n    eval_acc = 0.0\n\n    for i, data in enumerate(dataloader):\n        \n        if name in ['PACS']:\n            X = data['images']\n            y = torch.squeeze(data['labels'])\n\n            X = X.to(device)\n            y = y.to(device)\n\n        else:\n            X, y = data[0].to(device), data[1].to(device)\n        \n        logits = model(X)\n        loss = criterion(logits, y)\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += (y_pred == y).sum().item() / len(y)\n\n        # Print dynamic progress on the same line using \\r\n        print(f'\\rEvaluation: [{i+1}/{len(dataloader)}] '\n              f'Loss: {eval_loss / (i + 1):.4f} '\n              f'Acc: {eval_acc / (i + 1):.4f}', end='')\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    \n    # Move to the next line after the loop is done\n    print()  \n    \n    return eval_loss, eval_acc","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:09:33.309769Z","iopub.execute_input":"2024-09-28T08:09:33.310108Z","iopub.status.idle":"2024-09-28T08:09:33.324840Z","shell.execute_reply.started":"2024-09-28T08:09:33.310073Z","shell.execute_reply":"2024-09-28T08:09:33.323849Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport csv\nfrom tqdm import tqdm \nimport time as time\n\nout_dir = 'output'\nos.makedirs(out_dir, exist_ok=True)\nepochs = 3\nlr = 3e-4\n\nloaders = [normal_loader, edge_loader, color_loader, shape_loader]\nnames = ['normal', 'edge', 'color', 'stylized']\n\ntrain_dl = train_loader\n# test_dl = val_loader\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nbar_format = '{l_bar}{bar} | Epoch: {n_fmt}/{total_fmt} | Time: {elapsed} < {remaining} | {rate_fmt}'\nmodel.to(device)\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\", bar_format=bar_format, leave=True):\n    start_time = time.time()  # Track the start time of the epoch\n\n    train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device, 'custom')\n    val_loss, val_acc = eval_step(model, loaders[0], criterion, device, 'custom')\n\n    epoch_duration = time.time() - start_time\n\n    tqdm.write(f\"============ Epoch {epoch + 1} --> Train Acc: {train_acc:.4f} || Val Acc: {val_acc:.4f} || Time: {epoch_duration:.2f} s ============\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:16:52.371724Z","iopub.execute_input":"2024-09-28T08:16:52.372220Z","iopub.status.idle":"2024-09-28T08:16:52.377859Z","shell.execute_reply.started":"2024-09-28T08:16:52.372180Z","shell.execute_reply":"2024-09-28T08:16:52.376960Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"names = ['normal', 'edge', 'color', 'stylized', 'test']\nloaders.append(DataLoader(test_dataset , batch_size=BATCH_SIZE, shuffle=False, num_workers=2))\nnormal_loss, normal_acc = eval_step(model, loaders[0], criterion, device, 'custom')\n\nfor name, val_loader in zip(names[1:], loaders[1:]): \n    val_loss, val_acc = eval_step(model, val_loader, criterion, device, 'custom')\n    print(f'\\n=========== {name} accuracy: {val_acc*100:.3f}% || normal accuracy: {normal_acc*100:.3f}% ===========\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T08:17:30.595702Z","iopub.execute_input":"2024-09-28T08:17:30.596496Z","iopub.status.idle":"2024-09-28T08:17:50.835908Z","shell.execute_reply.started":"2024-09-28T08:17:30.596453Z","shell.execute_reply":"2024-09-28T08:17:50.834776Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Evaluation: [16/16] Loss: 0.0899 Acc: 0.9805\nEvaluation: [16/16] Loss: 1.7781 Acc: 0.4094\n\n=========== edge accuracy: 40.938% || normal accuracy: 98.047% ===========\n\nEvaluation: [16/16] Loss: 0.3008 Acc: 0.9207\n\n=========== color accuracy: 92.070% || normal accuracy: 98.047% ===========\n\nEvaluation: [16/16] Loss: 1.3855 Acc: 0.5711\n\n=========== stylized accuracy: 57.109% || normal accuracy: 98.047% ===========\n\nEvaluation: [123/123] Loss: 0.0713 Acc: 0.9878\n\n=========== test accuracy: 98.780% || normal accuracy: 98.047% ===========\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing the Scripts","metadata":{}},{"cell_type":"code","source":"import shutil \n\nREPO_ROOT = '../input/atml-repo'\ndest_path = './repo'\n\n# Make sure the destination directory exists\nos.makedirs(dest_path, exist_ok=True)\n\n# Copy the entire directory from REPO_ROOT to dest_path\nshutil.copytree(REPO_ROOT, dest_path, dirs_exist_ok=True)\n\nprint(f\"Folder copied from {REPO_ROOT} to {dest_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:09.594155Z","iopub.execute_input":"2024-09-27T19:34:09.594965Z","iopub.status.idle":"2024-09-27T19:34:09.654680Z","shell.execute_reply.started":"2024-09-27T19:34:09.594923Z","shell.execute_reply":"2024-09-27T19:34:09.653626Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Folder copied from ../input/atml-repo to ./repo\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile './repo/custom_infer.py'\nimport argparse\nimport time \nimport csv\nimport matplotlib.pyplot as plt \nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nfrom model import *\nfrom data import *\nfrom utils import *\n\ndef train_step(model, dataloader, criterion, optimizer, device):\n    '''Train for one epoch'''\n    \n    model.train()\n\n    train_loss = 0.0\n    train_acc = 0.0\n\n    for i, data in enumerate(dataloader):\n\n        X, y = data[0].to(device), data[1].to(device)\n        \n        logits = model(X)\n        loss = criterion(logits, y)\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        train_acc += (y_pred == y).sum().item() / len(y)\n\n        # Print dynamic progress on the same line using \\r\n        print(f'\\rTraining: [{i+1}/{len(dataloader)}] '\n              f'Loss: {train_loss / (i + 1):.4f} '\n              f'Acc: {train_acc / (i + 1):.4f}', end='')\n\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    \n    # Move to the next line after the loop is done\n    print()  \n    \n    return train_loss, train_acc\n\n@torch.inference_mode()\ndef eval_step(model, dataloader, criterion, device):\n    '''Evaluate the model'''\n    \n    model.eval()\n\n    eval_loss = 0.0\n    eval_acc = 0.0\n\n    for i, data in enumerate(dataloader):\n\n        X, y = data[0].to(device), data[1].to(device)\n        \n        logits = model(X)\n        loss = criterion(logits, y)\n        eval_loss += loss.item()\n\n        y_pred = torch.argmax(logits.detach(), dim=1)\n        eval_acc += (y_pred == y).sum().item() / len(y)\n\n        # Print dynamic progress on the same line using \\r\n        print(f'\\rEvaluation: [{i+1}/{len(dataloader)}] '\n              f'Loss: {eval_loss / (i + 1):.4f} '\n              f'Acc: {eval_acc / (i + 1):.4f}', end='')\n\n    eval_loss = eval_loss / len(dataloader)\n    eval_acc = eval_acc / len(dataloader)\n    \n    # Move to the next line after the loop is done\n    print()  \n    \n    return eval_loss, eval_acc\n\nif __name__ == \"__main__\":\n    \n    parser = argparse.ArgumentParser(description='Finetune a pretrained ResNet!')\n    parser.add_argument('--model_name', type=str, default='resnet-34', help='Choose from [resnet-34, vit or clip]. Default is resnet-34.')\n    parser.add_argument('--save_model', type=bool, help='Specify whether the trained model must be saved or not. Will be saved in {output_dir}/models.', default=True)\n    parser.add_argument('--out_dir', type=str, default='./output', help='Path to the directory where training log and model will be saved.')\n    parser.add_argument('--train_path', type=str, help='Path to training data.')\n    parser.add_argument('--val_path', type=str, help='Path to validation data.')\n    parser.add_argument('--lr', type=float, help='Learning Rate. Default is 1e-4', default=1e-4)\n    parser.add_argument('--batch_size', type=int, help='Batch size. Default is 32.', default=32)\n    parser.add_argument('--epochs', type=int, help='Number of fine-tuning epochs. Default is 15', default=15)\n    \n    args = parser.parse_args()\n\n    set_seed()\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    batch_size = args.batch_size\n    num_workers = 2\n\n    os.makedirs(f'{args.out_dir}/model', exist_ok=True)\n    os.makedirs(f'{args.out_dir}/log', exist_ok=True)\n\n    train_ds, test_ds = get_custom_data(args.train_path, args.val_path)\n\n    train_dl = get_dataloader(train_ds, batch_size, True, num_workers)\n    test_dl = get_dataloader(test_ds, batch_size, False, num_workers)\n\n    num_classes = len(train_ds.classes)\n\n    model = load_resnet_ft(args.model_name, 'classifier')\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n    print(f'\\nFinetuning {args.model_name} with {count_parameters(model) * 1e-6:.3f}M params for {args.epochs} epochs on {device}...\\n')\n\n    best_loss = float('inf')\n    bar_format = '{l_bar}{bar} | Epoch: {n_fmt}/{total_fmt} | Time: {elapsed} < {remaining} | {rate_fmt}'\n\n    with open(os.path.join(args.out_dir, 'log/run.csv'), 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(['epoch', 'train_loss', 'train_acc', 'test_loss', 'test_acc'])\n\n        for epoch in tqdm(range(args.epochs), desc=\"Epochs\", bar_format=bar_format, leave=True):\n            start_time = time.time()  # Track the start time of the epoch\n\n            train_loss, train_acc = train_step(model, train_dl, criterion, optimizer, device)\n            test_loss, test_acc = eval_step(model, test_dl, criterion, device)\n            \n            if test_loss < best_loss:\n                best_loss = test_loss\n                if args.save_model:\n                    torch.save(model.state_dict(), os.path.join(args.out_dir, 'model/best.pth'))\n\n            writer.writerow([epoch + 1, train_loss, train_acc, test_loss, test_acc])\n            \n            # Calculate epoch duration\n            epoch_duration = time.time() - start_time\n            \n            # Use tqdm.write to print epoch summary with duration\n            tqdm.write(f\"============ Epoch {epoch + 1} --> Train Acc: {train_acc:.4f} || Test Acc: {test_acc:.4f} || Time: {epoch_duration:.2f} s ============\\n\")\n            \n    # Save this model at the end of run (commented out for)\n    if args.save_model:\n        torch.save(model.state_dict(), os.path.join(args.out_dir, 'model/final.pth'))\n\n    df = pd.read_csv(f'{args.out_dir}/log/run.csv')\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plot Train Accuracy and Test Accuracy\n    ax[0].plot(df['train_acc'], label='Train Accuracy', color='orange', marker='o', linewidth=2)\n    ax[0].plot(df['test_acc'], label='Test Accuracy', color='blue', marker='o', linewidth=2)\n    ax[0].set_title('Train vs Test Accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_ylabel('Accuracy')\n    ax[0].legend()\n    ax[0].grid(True)\n\n    # Plot Train Loss and Test Loss\n    ax[1].plot(df['train_loss'], label='Train Loss', color='orange', marker='o', linewidth=2)\n    ax[1].plot(df['test_loss'], label='Test Loss', color='blue', marker='o', linewidth=2)\n    ax[1].set_title('Train vs Test Loss')\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].legend()\n    ax[1].grid(True)\n\n    plt.tight_layout()\n    plt.savefig(f'{args.out_dir}/log_plot.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:13.149712Z","iopub.execute_input":"2024-09-27T19:34:13.150161Z","iopub.status.idle":"2024-09-27T19:34:13.161757Z","shell.execute_reply.started":"2024-09-27T19:34:13.150114Z","shell.execute_reply":"2024-09-27T19:34:13.160779Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting ./repo/custom_infer.py\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dir = 'imagenette2/train'\n\nfor folder in os.listdir(train_dir):\n    if folder in class_map:\n        old_folder = os.path.join(train_dir, folder)\n        new_folder = os.path.join(train_dir, class_map[folder])\n        os.rename(old_folder, new_folder)\n        print(f'Renamed {folder} to {class_map[folder]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:17.686337Z","iopub.execute_input":"2024-09-27T19:34:17.687198Z","iopub.status.idle":"2024-09-27T19:34:17.693677Z","shell.execute_reply.started":"2024-09-27T19:34:17.687155Z","shell.execute_reply":"2024-09-27T19:34:17.692712Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Renamed n03000684 to Chainsaw\nRenamed n03888257 to Parachute\nRenamed n03417042 to Garbage Truck\nRenamed n03445777 to Golf Ball\nRenamed n01440764 to Tench\nRenamed n03028079 to Church\nRenamed n02979186 to Casette Player\nRenamed n03425413 to Gas Pump\nRenamed n03394916 to French Horn\nRenamed n02102040 to English Springer\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install clip","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:20.006280Z","iopub.execute_input":"2024-09-27T19:34:20.006634Z","iopub.status.idle":"2024-09-27T19:34:34.846757Z","shell.execute_reply.started":"2024-09-27T19:34:20.006602Z","shell.execute_reply":"2024-09-27T19:34:34.845452Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting clip\n  Downloading clip-0.2.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=27ac91963e86e2e953daee81ef453ca7ec8f916d389d80ee0e30d6272dd8746c\n  Stored in directory: /root/.cache/pip/wheels/7f/5c/e6/2c0fdb453a3569188864b17e9676bea8b3b7e160c037117869\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-0.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_path = 'imagenette2/train'\nval_data_path = 'edges'\n\nos.system(f'python repo/custom_infer.py --epochs 3 --out_dir script_output --lr 3e-4 --train_path {train_data_path} --val_path {val_data_path}')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:34.849044Z","iopub.execute_input":"2024-09-27T19:34:34.849956Z","iopub.status.idle":"2024-09-27T19:37:19.921618Z","shell.execute_reply.started":"2024-09-27T19:34:34.849904Z","shell.execute_reply":"2024-09-27T19:37:19.920631Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 176MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFreezing non-classifier head weights ....\n\n\nFinetuning resnet-34 with 0.513M params for 3 epochs on cuda...\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:   0%|           | Epoch: 0/3 | Time: 00:00 < ? | ?it/s","output_type":"stream"},{"name":"stdout","text":"Training: [296/296] Loss: 3.5074 Acc: 0.3403\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  33%|███▎       | Epoch: 1/3 | Time: 00:52 < 01:45 | 52.99s/it","output_type":"stream"},{"name":"stdout","text":"Evaluation: [16/16] Loss: 3.2103 Acc: 0.1398\n============ Epoch 1 --> Train Acc: 0.3403 || Test Acc: 0.1398 || Time: 52.99 s ============\n\nTraining: [296/296] Loss: 0.5288 Acc: 0.8442\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  67%|██████▋    | Epoch: 2/3 | Time: 01:45 < 00:52 | 52.67s/it","output_type":"stream"},{"name":"stdout","text":"Evaluation: [16/16] Loss: 3.4283 Acc: 0.1562\n============ Epoch 2 --> Train Acc: 0.8442 || Test Acc: 0.1562 || Time: 52.44 s ============\n\nTraining: [296/296] Loss: 0.2812 Acc: 0.9217\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████ | Epoch: 3/3 | Time: 02:38 < 00:00 | 52.69s/it\n","output_type":"stream"},{"name":"stdout","text":"Evaluation: [16/16] Loss: 3.5696 Acc: 0.1777\n============ Epoch 3 --> Train Acc: 0.9217 || Test Acc: 0.1777 || Time: 52.63 s ============\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
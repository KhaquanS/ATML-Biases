{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9503317,"sourceType":"datasetVersion","datasetId":5783864},{"sourceId":9503332,"sourceType":"datasetVersion","datasetId":5783873},{"sourceId":9503356,"sourceType":"datasetVersion","datasetId":5783893},{"sourceId":9503382,"sourceType":"datasetVersion","datasetId":5783905},{"sourceId":9503850,"sourceType":"datasetVersion","datasetId":5784265},{"sourceId":9503886,"sourceType":"datasetVersion","datasetId":5784297}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install git+https://github.com/openai/CLIP.git\n!pip install deeplake\n!git clone https://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@github.com/KhaquanS/ATML-Biases.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-30T13:50:31.714535Z","iopub.execute_input":"2024-09-30T13:50:31.714928Z","iopub.status.idle":"2024-09-30T13:51:24.783502Z","shell.execute_reply.started":"2024-09-30T13:50:31.714880Z","shell.execute_reply":"2024-09-30T13:51:24.782304Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%cd ATML-Biases\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:51:24.785304Z","iopub.execute_input":"2024-09-30T13:51:24.785632Z","iopub.status.idle":"2024-09-30T13:51:25.768655Z","shell.execute_reply.started":"2024-09-30T13:51:24.785597Z","shell.execute_reply":"2024-09-30T13:51:25.767677Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/ATML-Biases\nREADME.md\t custom_infer_vit.py  noise_inject.py\t   train_clip.py\nclip_infer.py\t data.py\t      scrambled_images.py  train_vit.py\ncustom_infer.py  model.py\t      train.py\t\t   utils.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Task 2: Evaluating CLIP on CIFAR10**","metadata":{}},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --dataset CIFAR-10","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:38:21.771231Z","iopub.execute_input":"2024-09-29T20:38:21.771650Z","iopub.status.idle":"2024-09-29T20:39:18.182598Z","shell.execute_reply.started":"2024-09-29T20:38:21.771598Z","shell.execute_reply":"2024-09-29T20:39:18.181447Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n100%|███████████████████████████████████████| 338M/338M [00:08<00:00, 40.6MiB/s]\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:03<00:00, 49554472.49it/s]\nExtracting ./output/CIFAR-10/cifar-10-python.tar.gz to ./output/CIFAR-10\nFiles already downloaded and verified\nEvaluation: [313/313] Loss: 462.7173 Acc: 0.8873\n============ Eval Acc on CIFAR-10: 88.7300%  ============\n============ Eval Loss on CIFAR-10: 462.7173  ============\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Task 3: Evaluating CLIP on CIFAR100, PACS, & SVHN**","metadata":{}},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --dataset PACS\n!python clip_infer.py --model_name clip-vit --dataset CIFAR-100\n!python clip_infer.py --model_name clip-vit --dataset SVHN","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:39:53.313308Z","iopub.execute_input":"2024-09-29T20:39:53.314286Z","iopub.status.idle":"2024-09-29T20:43:16.048467Z","shell.execute_reply.started":"2024-09-29T20:39:53.314239Z","shell.execute_reply":"2024-09-29T20:43:16.047458Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n\u001b[KOpening dataset in read-only mode as you don't have write permissions.\n\u001b[KThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-train\n\n\u001b[Khub://activeloop/pacs-train loaded successfully.\n\n\u001b[KOpening dataset in read-only mode as you don't have write permissions.\n\u001b[KThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-val\n\n\u001b[Khub://activeloop/pacs-val loaded successfully.\n\nEvaluation: [32/32] Loss: 327.1055 Acc: 0.9438\n============ Eval Acc on PACS: 94.3787%  ============\n============ Eval Loss on PACS: 327.1055  ============\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./output/CIFAR-100/MNIST/raw/train-images-idx3-ubyte.gz\n100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 17512921.74it/s]\nExtracting ./output/CIFAR-100/MNIST/raw/train-images-idx3-ubyte.gz to ./output/CIFAR-100/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./output/CIFAR-100/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|█████████████████████████████████| 28881/28881 [00:00<00:00, 459665.67it/s]\nExtracting ./output/CIFAR-100/MNIST/raw/train-labels-idx1-ubyte.gz to ./output/CIFAR-100/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./output/CIFAR-100/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 2030727.24it/s]\nExtracting ./output/CIFAR-100/MNIST/raw/t10k-images-idx3-ubyte.gz to ./output/CIFAR-100/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./output/CIFAR-100/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 3048572.37it/s]\nExtracting ./output/CIFAR-100/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./output/CIFAR-100/MNIST/raw\n\nEvaluation: [313/313] Loss: 474.0300 Acc: 0.3292\n============ Eval Acc on CIFAR-100: 32.9200%  ============\n============ Eval Loss on CIFAR-100: 474.0300  ============\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nDownloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./output/SVHN/train_32x32.mat\n100%|████████████████████████| 182040794/182040794 [00:39<00:00, 4643622.01it/s]\nDownloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./output/SVHN/test_32x32.mat\n100%|██████████████████████████| 64275384/64275384 [00:15<00:00, 4118503.55it/s]\nEvaluation: [814/814] Loss: 857.1941 Acc: 0.1873\n============ Eval Acc on SVHN: 18.7346%  ============\n============ Eval Loss on SVHN: 857.1941  ============\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Task 4: Evaluating on Shape, Texture, Color Biases**","metadata":{}},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --bias_eval \"texture\" --train_path \"/kaggle/input/content-imagenette-1/content_imagenette\" --val_path \"/kaggle/input/style-imagenette-1/stylized_imagenette\"\n!python clip_infer.py --model_name clip-vit --bias_eval \"edge\" --train_path \"/kaggle/input/content-imagenette-1/content_imagenette\" --val_path \"/kaggle/input/edge-imagenette-1/edge_imagenette\"\n!python clip_infer.py --model_name clip-vit --bias_eval \"color\" --train_path \"/kaggle/input/content-imagenette-1/content_imagenette\" --val_path \"/kaggle/input/color-imagenette-1/color_imagenette\"","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:45:16.263099Z","iopub.execute_input":"2024-09-29T20:45:16.264205Z","iopub.status.idle":"2024-09-29T20:46:15.880121Z","shell.execute_reply.started":"2024-09-29T20:45:16.264158Z","shell.execute_reply":"2024-09-29T20:46:15.879027Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nEvaluation: [16/16] Loss: 412.4688 Acc: 0.9800\n============ True Eval Acc: 98.0000%  ============\n============ True Eval Loss: 412.4688  ============\nEvaluation: [16/16] Loss: 566.4688 Acc: 0.6340\n============ Eval Acc on texture dataset: 63.4000%  ============\n============ Eval Loss on texture dataset: 566.4688  ============\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nEvaluation: [16/16] Loss: 412.4688 Acc: 0.9800\n============ True Eval Acc: 98.0000%  ============\n============ True Eval Loss: 412.4688  ============\nEvaluation: [16/16] Loss: 418.9219 Acc: 0.5560\n============ Eval Acc on edge dataset: 55.6000%  ============\n============ Eval Loss on edge dataset: 418.9219  ============\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nEvaluation: [16/16] Loss: 412.4688 Acc: 0.9800\n============ True Eval Acc: 98.0000%  ============\n============ True Eval Loss: 412.4688  ============\nEvaluation: [16/16] Loss: 514.6719 Acc: 0.9200\n============ Eval Acc on color dataset: 92.0000%  ============\n============ Eval Loss on color dataset: 514.6719  ============\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Task 5: Evaluating on Noise, Permutation Invariant, and Global Style Transfer CIFAR-10**","metadata":{}},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --bias_eval \"Noise\" --dataset CIFAR-10","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:36:46.190924Z","iopub.execute_input":"2024-09-29T21:36:46.191322Z","iopub.status.idle":"2024-09-29T21:38:50.307840Z","shell.execute_reply.started":"2024-09-29T21:36:46.191277Z","shell.execute_reply":"2024-09-29T21:38:50.306833Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 99.2MiB/s]\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/train/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:15<00:00, 11132153.24it/s]\nExtracting ./output/CIFAR-10/train/cifar-10-python.tar.gz to ./output/CIFAR-10/train\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/val/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:15<00:00, 11041775.10it/s]\nExtracting ./output/CIFAR-10/val/cifar-10-python.tar.gz to ./output/CIFAR-10/val\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/noise-val/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:15<00:00, 11169683.08it/s]\nExtracting ./output/CIFAR-10/noise-val/cifar-10-python.tar.gz to ./output/CIFAR-10/noise-val\nEvaluation: [313/313] Loss: 425.7045 Acc: 0.8208\n============ True Eval Acc: 82.0800%  ============\n============ True Eval Loss: 425.7045  ============\nEvaluation: [313/313] Loss: 393.6522 Acc: 0.8204\n============ Eval Acc on Noise dataset: 82.0400%  ============\n============ Eval Loss on Noise dataset: 393.6522  ============\n","output_type":"stream"}]},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --bias_eval \"Scrambled\" --dataset CIFAR-10","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:51:41.841032Z","iopub.execute_input":"2024-09-30T13:51:41.841962Z","iopub.status.idle":"2024-09-30T13:53:08.957886Z","shell.execute_reply.started":"2024-09-30T13:51:41.841919Z","shell.execute_reply":"2024-09-30T13:53:08.956925Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 93.6MiB/s]\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/train/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:04<00:00, 35658162.53it/s]\nExtracting ./output/CIFAR-10/train/cifar-10-python.tar.gz to ./output/CIFAR-10/train\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/val/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:03<00:00, 48834429.60it/s]\nExtracting ./output/CIFAR-10/val/cifar-10-python.tar.gz to ./output/CIFAR-10/val\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/CIFAR-10/scrambled-val/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:04<00:00, 40887687.12it/s]\nExtracting ./output/CIFAR-10/scrambled-val/cifar-10-python.tar.gz to ./output/CIFAR-10/scrambled-val\nEvaluation: [313/313] Loss: 425.7045 Acc: 0.8208\n============ True Eval Acc: 82.0800%  ============\n============ True Eval Loss: 425.7045  ============\nEvaluation: [313/313] Loss: 360.0196 Acc: 0.3294\n============ Eval Acc on Scrambled dataset: 32.9400%  ============\n============ Eval Loss on Scrambled dataset: 360.0196  ============\n","output_type":"stream"}]},{"cell_type":"code","source":"!python clip_infer.py --model_name clip-vit --bias_eval \"Global Style Transfer\" --train_path \"/kaggle/input/content-cifar10-2/content_cifar10\" --val_path \"/kaggle/input/stylized-cifar10-1/stylized_cifar10\"","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:13:22.685988Z","iopub.execute_input":"2024-09-29T22:13:22.686340Z","iopub.status.idle":"2024-09-29T22:13:40.220196Z","shell.execute_reply.started":"2024-09-29T22:13:22.686304Z","shell.execute_reply":"2024-09-29T22:13:40.219123Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\nEvaluation: [16/16] Loss: 437.1875 Acc: 0.9060\n============ True Eval Acc: 90.6000%  ============\n============ True Eval Loss: 437.1875  ============\nEvaluation: [16/16] Loss: 420.9688 Acc: 0.2940\n============ Eval Acc on Global Style Transfer dataset: 29.4000%  ============\n============ Eval Loss on Global Style Transfer dataset: 420.9688  ============\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Logging**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndataset = [\"CIFAR10\", \"CIFAR100\", \"PACS\", \"SVHN\", \"Imagenette\", \"Shape_Dataset\", \"Color_Dataset\", \"Texture_Dataset\", \"CIFAR10_Custom_Transform\", \"CIFAR10_Global_Subset\", \"Noisy_CIFAR10\", \"Global_Textured_CIFAR10\", \"Scrambled CIFAR10\"]\naccuracies = [88.73, 32.92, 94.37, 18.73, 98, 55.6, 92, 63.4, 82.08, 90.6, 82.04, 29.4, 32.94]\n\ndf = pd.DataFrame({\"Dataset\": dataset, \"Accuracy Score\":accuracies})\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:05:14.750264Z","iopub.execute_input":"2024-09-30T14:05:14.750956Z","iopub.status.idle":"2024-09-30T14:05:14.764935Z","shell.execute_reply.started":"2024-09-30T14:05:14.750916Z","shell.execute_reply":"2024-09-30T14:05:14.763936Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                     Dataset  Accuracy Score\n0                    CIFAR10           88.73\n1                   CIFAR100           32.92\n2                       PACS           94.37\n3                       SVHN           18.73\n4                 Imagenette           98.00\n5              Shape_Dataset           55.60\n6              Color_Dataset           92.00\n7            Texture_Dataset           63.40\n8   CIFAR10_Custom_Transform           82.08\n9      CIFAR10_Global_Subset           90.60\n10             Noisy_CIFAR10           82.04\n11   Global_Textured_CIFAR10           29.40\n12         Scrambled CIFAR10           32.94","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Accuracy Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CIFAR10</td>\n      <td>88.73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CIFAR100</td>\n      <td>32.92</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PACS</td>\n      <td>94.37</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SVHN</td>\n      <td>18.73</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Imagenette</td>\n      <td>98.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Shape_Dataset</td>\n      <td>55.60</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Color_Dataset</td>\n      <td>92.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Texture_Dataset</td>\n      <td>63.40</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CIFAR10_Custom_Transform</td>\n      <td>82.08</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CIFAR10_Global_Subset</td>\n      <td>90.60</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Noisy_CIFAR10</td>\n      <td>82.04</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Global_Textured_CIFAR10</td>\n      <td>29.40</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Scrambled CIFAR10</td>\n      <td>32.94</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv(\"log_clip.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:06:06.704210Z","iopub.execute_input":"2024-09-30T14:06:06.704965Z","iopub.status.idle":"2024-09-30T14:06:06.714297Z","shell.execute_reply.started":"2024-09-30T14:06:06.704929Z","shell.execute_reply":"2024-09-30T14:06:06.713493Z"},"trusted":true},"execution_count":8,"outputs":[]}]}
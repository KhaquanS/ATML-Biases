{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport time \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom typing import Optional, Tuple, Callable, Optional, Type, Union\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.models as models\n\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torchvision.transforms.v2 as transforms\nfrom torchvision import datasets\n\nimport torch.optim as optim\nimport csv\nfrom tqdm import tqdm \nimport time as time\n\ntorch.manual_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((32, 32)),  # Ensure all images are 32x32\n    transforms.ToTensor(),\n])\n\n# Download and create the CIFAR-10 train and test datasets\ntrain_ds = torchvision.datasets.CIFAR10(\n    root='./', train=True, download=True, transform=transform\n)\n\ntest_ds = torchvision.datasets.CIFAR10(\n    root='./', train=False, download=True, transform=transform\n)\n\nprint(f'Train Samples: {len(train_ds)} || Test Samples: {len(test_ds)} || Classes: {len(train_ds.classes)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_chans: int, out_chans: int, kernel_size: int, stride: int, padding: int):\n        super(DepthwiseSeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(in_chans, in_chans, kernel_size=kernel_size, stride=stride,\n                                   padding=padding, groups=in_chans, bias=False)\n        self.pointwise = nn.Conv2d(in_chans, out_chans, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\nclass PreNormAttention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        kernel_size: int = 3\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.q = nn.Linear(dim, dim)\n        self.k = nn.Linear(dim, dim)\n        self.v = nn.Linear(dim, dim)\n        self.proj = nn.Linear(dim, dim)\n        \n        # Depthwise convolution\n        self.depthwise_conv = DepthwiseSeparableConv(dim, dim, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n                \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, P, C = x.shape\n        H = self.num_heads\n        \n        q = self.q(x).view(B, P, H, -1).transpose(1, 2)\n        k = self.k(x).view(B, P, H, -1).transpose(1, 2)\n        v = self.v(x).view(B, P, H, -1).transpose(1, 2)\n        \n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        \n        x_reshaped = x.transpose(1, 2).view(B, C, int(P**0.5), int(P**0.5))\n        conv_out = self.depthwise_conv(x_reshaped)\n        \n        self.conv_feature_maps = conv_out.detach()  # Store conv feature maps for visualization\n        \n        conv_out = conv_out.view(B, C, P).transpose(1, 2)\n        \n        x = attn @ v\n        x = x.transpose(1, 2).reshape(B, P, C)\n        x = x + conv_out\n        \n        x = F.layer_norm(x, [C])\n        x = self.proj(x)\n        return x\n\nclass PatchEmbed(nn.Module):\n    def __init__(self,\n                 img_size: int,\n                 patch_size: int,\n                 in_chans: int,\n                 embed_dim: int,\n    ):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (self.img_size // self.patch_size, ) * 2\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n        self.proj = nn.Conv2d(in_chans,\n                              embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size,\n                              padding=0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1,2)\n        return x\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, num_patches: int, embed_dim: int):\n        super().__init__()\n        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x + self.position_embeddings\n\nclass Attention(nn.Module):\n    def __init__(self, img_size: int, patch_size: int, in_channels: int, embed_dim: int, num_heads: int):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n        self.positional_embedding = PositionalEmbedding(self.patch_embed.num_patches, embed_dim)\n        self.combined_attn = PreNormAttention(embed_dim, num_heads)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        x = self.patch_embed(x)  # Shape: (B, num_patches, embed_dim)\n        x = self.positional_embedding(x)\n        x = self.combined_attn(x)  # Shape: (B, num_patches, embed_dim)\n\n        return x\n    \nclass PreNormAttentionModel(nn.Module):\n    def __init__(self, img_size: int, patch_size: int, in_channels: int, embed_dim: int, num_heads: int, classes: int = 10):\n        super().__init__()     \n        self.attn = Attention(img_size, patch_size, in_channels, embed_dim, num_heads)\n        \n        num_patches = (img_size // patch_size) ** 2\n        self.fc = nn.Linear(num_patches * embed_dim, classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.attn(x)\n        B, P, C = x.shape\n        \n        x = x.view(B, -1)\n        x = self.fc(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]}]}